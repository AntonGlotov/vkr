import json
import nltk
import numpy as np
import requests
import re
import streamlit as st
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

st.set_page_config(
    page_title="–°–∞–π—Ç",
    page_icon="üíª",
    layout="centered",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://my-support.com'
    }
)

st.write('# –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é')


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –∏ –ø–æ–¥—Å—á–µ—Ç–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
def fetch_article_and_code_count(url):
    try:
        ua = UserAgent()

        headers = {
            'accept': 'application/json, text/plain, */*',
            'user-Agent': ua.google,
        }

        response = requests.get(url, headers=headers)
        response.raise_for_status()

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        soup = BeautifulSoup(response.text, 'lxml')

        # –ü–æ–∏—Å–∫ –∏ –ø–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
        code_blocks = soup.find_all(['code', 'pre'])
        code_lines_count = sum(block.get_text().count('\n') + 1 for block in code_blocks)

        # –£–¥–∞–ª–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
        for code_block in code_blocks:
            code_block.decompose()

        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏
        article_content = soup.find('div',
                                    class_='article-formatted-body article-formatted-body article-formatted-body_version-1')
        if not article_content:
            article_content = soup.find('div',
                                        class_='article-formatted-body article-formatted-body article-formatted-body_version-2')

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –µ–≥–æ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
        if article_content:
            article_text = ' '.join(article_content.stripped_strings)
        else:
            article_text = ""

        return article_text, code_lines_count
    except Exception as e:
        print(f"An error occurred: {e}")
        return "", 0


def fetch_urls_and_articles(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        data1 = json.load(f)
    return data1


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è URL –∏–∑ —Ñ–∞–π–ª–∞
def fetch_urls(file_path):
    # –û—Ç–∫—Ä—ã—Ç–∏–µ —Ñ–∞–π–ª–∞ –∏ —á—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)  # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö URL –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    urls = list(data.values())

    return urls


# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
urls = fetch_urls('C:/–£–Ω–∏–≤–µ—Ä/3 –∫—É—Ä—Å/–ö—É—Ä—Å–æ–≤–∞—è 2/Parcer1/articles_11_05_2025.json')

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
article_texts_and_code_counts = {url: fetch_article_and_code_count(url) for url in urls}

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
article_texts = {url: text for url, (text, _) in article_texts_and_code_counts.items()}
code_counts = {url: code_count for url, (_, code_count) in article_texts_and_code_counts.items()}

# –ó–∞–ø–∏—Å—å —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ JSON —Ñ–∞–π–ª
# with open(f"articlesJ.json", "w", encoding='utf-8') as f:
#     try:
#         json.dump(article_texts_and_code_counts, f, indent=4, ensure_ascii=False)
#         print('–°—Ç–∞—Ç—å–∏ –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã')
#     except:
#         print('–°—Ç–∞—Ç—å–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å')

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words('russian'))


def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return ' '.join(tokens)


processed_articles = {url: preprocess_text(content) for url, content in article_texts.items()}

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ç–µ–º —Å –ø–æ–º–æ—â—å—é TF-IDF
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(processed_articles.values())
tfidf_feature_names = vectorizer.get_feature_names_out()

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
code_counts_array = np.array([code_counts[url] for url in processed_articles.keys()]).reshape(-1, 1)
features_matrix = np.hstack((tfidf_matrix.toarray(), code_counts_array))


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def get_keywords(tfidf_matrix, feature_names, top_n=10):
    keywords = {}
    for idx in range(tfidf_matrix.shape[0]):
        article = tfidf_matrix[idx].toarray().flatten()
        scores = article
        sorted_indices = scores.argsort()[::-1]
        top_feature_indices = sorted_indices[:top_n]
        keywords[list(processed_articles.keys())[idx]] = [feature_names[i] for i in top_feature_indices]
    return keywords


# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
def get_recommendations(article_url, features_matrix, article_list, top_n=3):
    article_idx = article_list.index(article_url)
    cosine_similarities = cosine_similarity(features_matrix[article_idx:article_idx + 1], features_matrix).flatten()
    print(cosine_similarities)
    similar_indices = cosine_similarities.argsort()[::-1][1:top_n + 1]
    recommendations = [article_list[i] for i in similar_indices]
    return recommendations


def cosine_sim_matrix():
    keywords = get_keywords(tfidf_matrix, tfidf_feature_names)

    user_url = st.text_input('')
    fetch_article_and_code_count(user_url)

    array = []
    list_of_articles = fetch_urls_and_articles('C:/–£–Ω–∏–≤–µ—Ä/3 –∫—É—Ä—Å/–ö—É—Ä—Å–æ–≤–∞—è 2/Parcer1/articles_11_05_2025.json')
    print(list_of_articles)
    for key, values in list_of_articles.items():
        dict = {}
        dict['ID'] = re.findall(r'\d+', values)
        dict['name'] = key
        dict['–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'] = keywords[values]
        dict['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞'] = code_counts[values]
        array.append(dict)

    with open("recommend_art.json", "w", encoding='utf-8') as f:
        try:
            json.dump(array, f, indent=4, ensure_ascii=False)
            print('–°—Ç–∞—Ç—å–∏ –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã')
        except:
            print('–°—Ç–∞—Ç—å–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å')

    print(len(array))


# –ü—Ä–∏–º–µ—Ä
# article_url = urls[0]
# recommendations = get_recommendations(article_url, features_matrix, list(processed_articles.keys()))
#
# # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è {article_url}: {keywords[article_url]}")
# print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {recommendations}")
# print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ : {code_counts[article_url]}")

if __name__ == '__main__':
    # –í—ã–≤–æ–¥ –≤—Å–µ–≥–æ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
    keywords = get_keywords(tfidf_matrix, tfidf_feature_names)
    for url in urls:
        print(f"\nURL —Å—Ç–∞—Ç—å–∏: {url}")
        print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords[url]}")
        print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {get_recommendations(url, features_matrix, list(processed_articles.keys()))}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞: {code_counts[url]}")

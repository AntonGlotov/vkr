import json
import nltk
import numpy as np
import requests
import re
import sys
import types
import base64
import time
import sqlite3
import streamlit as st
import webbrowser
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import sklearn170
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from parcing import extract_keywords, find_name

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

stop_words = set(stopwords.words('russian'))

st.set_page_config(
    page_title="–°–∞–π—Ç",
    page_icon="üíª",
    layout="centered",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://my-support.com'
    }
)

# st.markdown(
#     """
#     <style>
#     body, .stTextInput, .stSelectbox, .stSlider, .stMarkdown, .stButton, .stDataFrame {
#         color: #000000 !important;
#     .stApp {
#         background-color: #FFFFFF;
#
#     }
#     </style>
#     """,
#     unsafe_allow_html=True
# )

st.write('# –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é')


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ –∏ –ø–æ–¥—Å—á–µ—Ç–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
def fetch_article_and_code_count(url):
    try:
        ua = UserAgent()

        headers = {
            'accept': 'application/json, text/plain, */*',

            'user-Agent': ua.google,
        }

        response = requests.get(url, headers=headers)

        response.raise_for_status()

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        soup = BeautifulSoup(response.text, 'lxml')

        # –ü–æ–∏—Å–∫ –∏ –ø–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
        code_blocks = soup.find_all(['code', 'pre'])
        code_lines_count = sum(block.get_text().count('\n') + 1 for block in code_blocks)

        # –£–¥–∞–ª–µ–Ω–∏–µ –±–ª–æ–∫–æ–≤ –∫–æ–¥–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
        for code_block in code_blocks:
            code_block.decompose()

        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏
        article_content = soup.find('div',
                                    class_='article-formatted-body article-formatted-body article-formatted-body_version-1')

        if not article_content:
            article_content = soup.find('div',
                                        class_='article-formatted-body article-formatted-body article-formatted-body_version-2')

        hub_links = soup.find_all('a', class_='tm-publication-hub__link')

        hubs = ''
        pattern = r'^–±–ª–æ–≥\b'
        for link in hub_links:
            hub_name = link.find('span').get_text(strip=True)
            if not re.match(pattern, hub_name, re.IGNORECASE):
                hubs += hub_name
                hubs += ' '

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –µ–≥–æ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
        if article_content:
            article_text = ' '.join(article_content.stripped_strings)
        else:
            article_text = ""
        if hubs and article_text and code_blocks:
            return str(article_text), code_lines_count, hubs
        else:
            return '', 0, ''

    except Exception as e:
        print(f"An error occurred: {e}")
        return "", 0


def fetch_urls_and_articles(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        data1 = json.load(f)

    return data1


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è URL –∏–∑ —Ñ–∞–π–ª–∞
def fetch_urls(file_path):
    # –û—Ç–∫—Ä—ã—Ç–∏–µ —Ñ–∞–π–ª–∞ –∏ —á—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)  # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö URL –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    urls = list(data.values())

    return urls


def download_data():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    urls = fetch_urls('C:/–£–Ω–∏–≤–µ—Ä/3 –∫—É—Ä—Å/–ö—É—Ä—Å–æ–≤–∞—è 2/Parcer1/articles_12_05_2025.json')

    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
    article_texts_and_code_counts = {url: fetch_article_and_code_count(url) for url in urls}

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
    article_texts = {url: text for url, (text, _) in article_texts_and_code_counts.items()}
    code_counts = {url: code_count for url, (_, code_count) in article_texts_and_code_counts.items()}

    return article_texts, code_counts


# –ó–∞–ø–∏—Å—å —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ JSON —Ñ–∞–π–ª
# with open(f"articlesJ.json", "w", encoding='utf-8') as f:
#     try:
#         json.dump(article_texts_and_code_counts, f, indent=4, ensure_ascii=False)
#         print('–°—Ç–∞—Ç—å–∏ –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã')
#     except:
#         print('–°—Ç–∞—Ç—å–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å')


def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

    return ' '.join(tokens)


def tfidf_keywords():
    article_texts, code_counts = download_data()
    processed_articles = {url: preprocess_text(content) for url, content in article_texts.items()}

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ç–µ–º —Å –ø–æ–º–æ—â—å—é TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(processed_articles.values())
    tfidf_feature_names = vectorizer.get_feature_names_out()

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞
    code_counts_array = np.array([code_counts[url] for url in processed_articles.keys()]).reshape(-1, 1)
    features_matrix = np.hstack((tfidf_matrix.toarray(), code_counts_array))

    return processed_articles, features_matrix, code_counts_array


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def get_keywords(tfidf_matrix, feature_names, top_n=10):
    processed_articles, features_matrix, code_counts_array = tfidf_matrix
    keywords = {}

    for idx in range(tfidf_matrix.shape[0]):
        article = tfidf_matrix[idx].toarray().flatten()
        scores = article
        sorted_indices = scores.argsort()[::-1]
        top_feature_indices = sorted_indices[:top_n]
        keywords[list(processed_articles.keys())[idx]] = [feature_names[i] for i in top_feature_indices]

    return keywords


# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
def get_recommendations(article_url, features_matrix, article_list, top_n=3):
    article_idx = article_list.index(article_url)
    cosine_similarities = cosine_similarity(features_matrix[article_idx:article_idx + 1], features_matrix).flatten()

    print(cosine_similarities)
    similar_indices = cosine_similarities.argsort()[::-1][1:top_n + 1]
    recommendations = [article_list[i] for i in similar_indices]

    return recommendations


# st.markdown("""
# <style>
#     .stTextInput>div>div>input {
#         color: black;
#         background-color: white;
#     }
# </style>
# """, unsafe_allow_html=True)

def is_habr_url(url):
    pattern = r'^https://habr\.com/ru/([a-zA-Z0-9\-_/]+/)?[0-9]{6}/?$'
    return bool(re.fullmatch(pattern, url))


correct_url = False

user_input_url = st.text_input('')
if not is_habr_url(user_input_url) and user_input_url:
    correct_url = False
    st.write('–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –∞–¥—Ä–µ—Å—Å')

else:
    correct_url = True
# def func(user_input_url):
if user_input_url and correct_url:
    con = sqlite3.connect('C:/–£–Ω–∏–≤–µ—Ä/3 –∫—É—Ä—Å/–ö—É—Ä—Å–æ–≤–∞—è 2/Parcer1/my_database.sqlite3')
    cursor = con.cursor()
    cursor.execute('SELECT name, keywords, code_count FROM Articles WHERE link=?', (user_input_url,))

    results = cursor.fetchall()
    if results:
        name, key, code = results[0][0], results[0][1], results[0][2]
        sklearn170.tf_idf_matrix(name, key, code)

    else:
        text, code = fetch_article_and_code_count(user_input_url)
        key = extract_keywords(text)
        name = find_name(user_input_url)

    words = key.split()

    result = ', '.join(words[:-1]) + ' ' + words[-1]
    st.write(f'–ù–∞–∑–≤–∞–Ω–∏–µ: {name}')
    st.write(f'–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ : {result}')
    st.write(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ : {code}')

    st.markdown("<h1 style='text-align: center;'>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h1>", unsafe_allow_html=True)

    recomend = sklearn170.tf_idf_matrix(name, key, code)
    # st.write(recomend)
    recommendations = json.loads(recomend)

    recs = []
    numbers = ["first", "second", "third", "fourth", "fifth", "sixth"]
    for position in numbers:
        cursor.execute('SELECT name, link, keywords, code_count FROM Articles WHERE short_link=?',
                       (recommendations[position],))
        rec = cursor.fetchall()
        recs.append(rec)

    recomendation = [x for x in recs if x]

    i = 1

    while i < len(recomendation) and i <= 4:

        if recomendation[i][0][1] == user_input_url:
            del recomendation[i][0]
            i += 1

        words = recomendation[i][0][2].split()
        result1 = ', '.join(words[:-1]) + ' ' + words[-1]

        st.markdown(
            f'<a href="{recomendation[i][0][1]}" style="color: #FFFFFF; font-weight: bold;">{i-1} {recomendation[i][0][0]}</a>',
            unsafe_allow_html=True
        )
        st.write(
            f"""
            <div style='color: #FFFFFF;'>
                –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {result1}<br>
                –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞: {recomendation[i][0][3]}
            </div>
            """,
            unsafe_allow_html=True
        )
        st.write('')
        i += 1


def dump_json_matrix(user_url):
    con = sqlite3.connect('C:/–£–Ω–∏–≤–µ—Ä/3 –∫—É—Ä—Å/–ö—É—Ä—Å–æ–≤–∞—è 2/Parcer1/my_database.sqlite3')
    cursor = con.cursor()
    con.execute('SELECT keywords FROM Articles')
    x = cursor.fetchall()
    con.close()
    keyword = extract_keywords(x)

    print(fetch_article_and_code_count(user_url))
    get_recommendations(user_url, x, keyword)
    array = []
    list_of_articles = fetch_urls_and_articles('C:/–£–Ω–∏–≤–µ—Ä/3 –∫—É—Ä—Å/–ö—É—Ä—Å–æ–≤–∞—è 2/Parcer1/articles_12_05_2025.json')
    print(list_of_articles)
    for key, values in list_of_articles.items():
        dict = {}
        dict['ID'] = values[17::]
        dict['name'] = key
        dict['–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'] = keyword[values]
        # dict['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞'] = code_counts[values]
        array.append(dict)
    get_recommendations(user_url, array, keyword)
    with open("recommend_art.json", "w", encoding='utf-8') as f:
        try:
            json.dump(array, f, indent=4, ensure_ascii=False)
            print('success')
        except:
            print('error')

    print(len(array))


# if __name__ == '__main__':
#     start = time.time()
#     get_recommendations('https://habr.com/ru/companies/amvera/articles/907990/')
#     end = time.time()
#     print(end-start)

# –ü—Ä–∏–º–µ—Ä
# article_url = urls[0]
# recommendations = get_recommendations(article_url, features_matrix, list(processed_articles.keys()))
#
# # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è {article_url}: {keywords[article_url]}")
# print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {recommendations}")
# print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –≤ : {code_counts[article_url]}")

# if __name__ == '__main__':
#     # –í—ã–≤–æ–¥ –≤—Å–µ–≥–æ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
#     keywords = get_keywords(tfidf_matrix, tfidf_feature_names)
#     for url in urls:
#         print(f"\nURL —Å—Ç–∞—Ç—å–∏: {url}")
#         print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords[url]}")
#         # print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {get_recommendations(url, features_matrix, list(processed_articles.keys()))}")
#         print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞: {code_counts[url]}")
